# Caution: use only the 'suggest_type' in case of using optuna
save_best: False
plot_agents: False
collect_stats: True
visualize: False

log_dir: ./tmp
video_dir: ${log_dir}/videos

hydra:
  run:
    dir: ${log_dir}/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}

optuna:
  study:
    _target_: optuna.create_study
    study_name: ddqn_vec
    direction: maximize
    pruner:
      _target_: optuna.pruners.MedianPruner
      n_startup_trials: 5
      n_warmup_steps: 5
      interval_steps: 1
  optimize:
    n_trials: 100
    timeout: 6000
    n_jobs: 1

logger:
  classname: bbrl.utils.logger.TFLogger
  log_dir: ${log_dir}
  cache_size: 10000
  every_n_seconds: 10
  verbose: False

gym_env:
  env_name: LunarLander-v2
  render_mode: rgb_array

algorithm:
  architecture:
    hidden_sizes: [512, 512]

  seed:
    train: 1
    eval: 99
    q: 1
    explorer: 456
    torch: 789

  explorer:
    epsilon_start: 
      suggest_type: float
      low: 0.5
      high: 0.75
    epsilon_end:
      suggest_type: float
      low: 0.1
      high: 0.2
    decay:
      suggest_type: float
      low: 0.90
      high: 0.99

  buffer:
    max_size: 
      suggest_type: categorical
      choices:
        - 5_000
        - 10_000
        - 25_000
        - 50_000
        - 100_000
    batch_size: 500
    learning_starts: 2000

  target_critic_update_interval: 
    suggest_type: categorical
    choices:
      - 5
      - 25
      - 50
      - 100
  max_grad_norm: 1.5

  nb_evals: 10
  n_envs: 5
  n_steps_train: 50

  optim_n_updates: 3
  discount_factor:
    suggest_type: float
    high: 0.95
    low: 0.8

  n_steps: 60_000
  eval_interval: 1000


optimizer:
  classname: torch.optim.Adam
  lr: 0.001
